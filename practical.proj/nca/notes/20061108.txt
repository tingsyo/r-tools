2006.11.08

[Self-generated data: RING3]
  - 100 data points, 50 for class=0, and 50 for class=1.
  - 2 variables, x1, x2, generated by multivariate normal distribution.
    # For class=0
    mu0 <- c(-1,-1)
    sigma0 <- matrix(c(1.5,0.7,0.7,0.5),2,2)
    # For class=1
    mu1 <- c(1,1)
    sigma1 <- matrix(c(0.5,-0.4,-0.4,1.5),2,2)

  - The resulting data is shown in "ring3.ps".

[Results for RING3 data]
------------------------------------------------------
      1NN NCA-COV NCA-PCOV NCA-COV-rd1 NCA-PCOV-rd1
------------------------------------------------------
 [1,] 0.0     0.0      0.0         
 [2,] 0.1     0.0      0.1         
 [3,] 0.1     0.1      0.1         
 [4,] 0.3     0.3      0.3         
 [5,] 0.1     0.1      0.1         
 [6,] 0.2     0.2      0.2         
 [7,] 0.1     0.2      0.1         
 [8,] 0.1     0.0      0.1         
 [9,] 0.5     0.4      0.5         
[10,] 0.1     0.1      0.1         
-------------------------------------------------------
ave.  0.16    0.14     0.16        
-------------------------------------------------------


[Note]
 1. NCA-COV outperforms other methods, and the transformed data is shown in "ring3cov.ps". (transformation matrix learned from whole dataset)

 2. I found another default function in R with do the Cholesky decomposition: chol(Q) = A, s.t., AA^T = Q.
    This matrix is not the same as the one obtained from eigen-value decomposition (A'), while AA'^T = A'^TA = Q.
    The results in optimization is the same for both A and A', since only Q is used during computating the probability distribution.  However, the results for dimension reduction will be different.
    
    
